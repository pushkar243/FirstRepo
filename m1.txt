    lock_file = os.path.join(data_dir, ".processing.lock")

    # Prevent double execution
    if os.path.exists(lock_file):
        print("Another process is already running. Exiting.")
        return

    try:
        # Create lock file
        with open(lock_file, "w") as f:
            f.write("processing")


 finally:
        # Always remove lock file
        if os.path.exists(lock_file):
            os.remove(lock_file)
----------------------------
import os
import shutil

def wrapper():
    project_dir = "/opt/airflow/dags/Lang_Models/taxonomy_data_xml_IBResearch/"
    project_datafiles_dir = "/opt/airflow/datalake/cid/staat_wro/taxonomy_data_xml_IBResearch/"
    python_dir = project_dir + "python"
    cert_dir = project_datafiles_dir + "certificates"
    models_dir = project_datafiles_dir + "models"
    data_dir = project_datafiles_dir + "data"
    processed_dir = os.path.join(data_dir, "processed")

    # Create processed directory if not exists
    os.makedirs(processed_dir, exist_ok=True)

    # Loop through files in the data directory
    for file in os.listdir(data_dir):
        if file.endswith(".pub.xml"):
            base_name = file.replace(".pub.xml", "")

            first_xml_path = os.path.join(data_dir, f"{base_name}.pub.xml")
            second_xml_path = os.path.join(data_dir, f"{base_name}.txt.xml")

            if not os.path.exists(second_xml_path):
                print(f"Skipping {base_name} - second XML not found")
                continue

            try:
                # Extract metadata from the first XML
                extracted_data = extract_metadata_from_first_xml(first_xml_path)

                # Extract text content from the second XML
                extracted_text = extract_text_from_second_xml(second_xml_path)

                # Add parsed text
                extracted_data["parsed_text"] = extracted_text

                # Print (or process) results
                print(f"Extracted Data for {base_name}:")
                print(extracted_data)

                # Insert into DB
                insert_or_replace(connection_string, table_name, extracted_data)

                # Move processed files
                shutil.move(first_xml_path, os.path.join(processed_dir, f"{base_name}.pub.xml"))
                shutil.move(second_xml_path, os.path.join(processed_dir, f"{base_name}.txt.xml"))

                print(f"Moved {base_name} files to processed folder.")

            except Exception as e:
                print(f"Error processing {base_name}: {e}")


********************************************************************************
dedup_query = f"""
WITH ranked_rows AS (
    SELECT 
        ctid,
        ROW_NUMBER() OVER (
            PARTITION BY id
            ORDER BY timestamp DESC
        ) AS rn
    FROM {SCHEMA}.{GP_OUTPUT_MODEL}
)
DELETE FROM {SCHEMA}.{GP_OUTPUT_MODEL}
WHERE ctid IN (
    SELECT ctid FROM ranked_rows WHERE rn > 1
);
"""

cursor.execute(dedup_query)
conn.commit()
******************************************************************************

def get_no_pages(date, cert_dir):
    CERT = get_cert(cert_dir)
    PAYLOAD = get_payload(date)
    MAX_RETRIES = 5
    RETRY_WAIT = 180  # seconds

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            logger.info(f"Attempt {attempt}: Requesting page data for date {date}")
            response = requests.get(PAYLOAD, headers=cfg.HEADERS, cert=CERT, verify=False)
            response.raise_for_status()
            response_d = json.loads(response.content)
            no_pages = response_d.get("page", {}).get("totalPages")
            if no_pages is not None:
                return no_pages
            else:
                raise ValueError("Missing 'totalPages' in response")
        except Exception as e:
            logger.warning(f"[Attempt {attempt}] Error for date {date}: {e}")
            if attempt == MAX_RETRIES:
                logger.error(f"Max retries exhausted. Returning 0 pages for date {date}")
                return 0  # ← gracefully handle instead of raising
            else:
                logger.info(f"Retrying in {RETRY_WAIT} seconds for date {date}...")
                time.sleep(RETRY_WAIT)


******************************************

# ✅ Restore privileges
db_con.execute(
    f"GRANT SELECT ON TABLE {cfg.SCHEMA}.{cfg.GP_OUTPUT_MODEL} TO PUBLIC;"
)

*******************************************************************************************

update query 
def _upload_to_db(data_dir, db_con):
    """Performs final cleaning-up and upload stories to GP"""

    # Load data
    df = pd.read_pickle(f"{data_dir}/stories_with_meta_tagged_pdfs_ISIN.pickle")

    df["portal_article_href"] = (
        "https://bw.gpwb.ubs.com/app/NQS/impact/detail/" + df["id"].astype(str)
    )
    df["id"] = "J-" + df["id"].astype(str)

    date_now = datetime.today().strftime("%Y-%m-%d %H:%M")
    print(date_now)
    df["as_of_date"] = date_now
    df = df.replace("NULL", np.nan)

    # Clean columns with list of text or text-list types
    for c in cfg.COLUMNS_TEXT_TYPE:
        df[c] = df[c].apply(lambda x: np.nan if x == ["NULL"] else x)

    for c in cfg.COLUMNS_TEXT_LIST_TYPE:
        df[c] = df[c].apply(lambda x: " || ".join(x) if isinstance(x, typing.List) else x)

    print("df before upload: ", df.shape)

    # Step 1: Truncate temp table
    db_con.execute(f"TRUNCATE TABLE {cfg.SCHEMA}.temp_table")

    # Step 2: Upload data to temp table (preserve structure & ownership)
    df.to_sql(
        name="temp_table",
        con=db_con,
        index=False,
        if_exists="append",  # ✅ safer than replace
        schema=cfg.SCHEMA,
        chunksize=10000,
    )

    # Step 3: Build and run UPSERT query dynamically
    columns = df.columns.tolist()
    columns_str = ", ".join(columns)
    update_str = ",\n    ".join(
        [f"{col} = EXCLUDED.{col}" for col in columns if col != "id"]
    )

    upsert_query = f"""
    INSERT INTO {cfg.SCHEMA}.{cfg.GP_OUTPUT_MODEL} ({columns_str})
    SELECT {columns_str}
    FROM {cfg.SCHEMA}.temp_table
    ON CONFLICT (id)
    DO UPDATE SET
        {update_str};
    """

    print("Running UPSERT...")
    db_con.execute(upsert_query)

    # Step 4: Optional deduplication step
    print("deduplication")
    db_con.execute(cfg.DEDUP_QUERY)


***********************************************************************************************************************************************
from collections import defaultdict
from typing import List
from langchain_core.documents import Document

def get_intentions(question: str) -> List[str]:
    """
    Perform hybrid semantic search with reranking on the question,
    then extract the most relevant insight_type values from the results.

    Args:
        question (str): The search query.

    Returns:
        List[str]: Top insight_type values within 2% of the top reranker score.
    """
    # Get the vectorstore instance
    vs = get_vectorstore()

    # Perform hybrid search with reranking
    docs_and_scores = vs.semantic_hybrid_search_with_score_and_rerank(
        question,
        **{"k": settings.search_k}
    )

    # Group reranker scores by insight_type
    reranker_scores = defaultdict(list)

    for doc, relevance_score, reranker_score in docs_and_scores:
        metadata = doc.metadata
        insight_type = metadata.get("insight_type")
        if insight_type and reranker_score is not None:
            reranker_scores[insight_type].append(reranker_score)

    # Calculate max reranker score for each insight_type
    max_scores = {
        insight_type: max(scores)
        for insight_type, scores in reranker_scores.items()
    }

    if not max_scores:
        return []

    top_score = max(max_scores.values())
    threshold = top_score * 0.98  # 2% below top score

    # Filter insight_types within threshold
    filtered_insight_types = [
        insight_type for insight_type, score in max_scores.items()
        if score >= threshold
    ]

    print("Intentions:", question)
    print("Docs and scores:", docs_and_scores)
    print("Filtered insight types:", filtered_insight_types)

    return filtered_insight_types
-----------------------
def build_combined_query(intentions, fa_name=None):
    try:
        if not intentions or not isinstance(intentions, list):
            raise ValueError("Intentions must be a non-empty list of lists.")

        query_blocks = []

        for group in intentions:
            if not group:
                continue

            for intent in group:
                intention_filter = f"insight_type ILIKE '%%{intent}%%'"

                fa_filter = ""
                if fa_name and fa_name != "FA_NOT_MENTIONED":
                    fa_filter = f" AND array_to_string(fa_names, ',') ILIKE '%%{fa_name}%%'"

                query_block = f"""
                    (
                        SELECT 
                            insight_type, fa_names, acc_nm, included_clients,
                            REGEXP_REPLACE(narrative_raw, '<[^>]+>', '', 'g') AS narrative_raw
                        FROM {settings.psql_schema}.{settings.psql_table}
                        WHERE {intention_filter}{fa_filter}
                        LIMIT 10
                    )
                """.strip()

                query_blocks.append(query_block)

        combined_query = "\nUNION ALL\n".join(query_blocks)
        print("Final SQL:\n", combined_query)
        return combined_query

    except Exception as e:
        logger.exception(f"Error in building combined query. Error: {e}")
        return None
**************************************************************************************


def build_combined_query(intentions, fa_name=None):
    try:
        if not intentions or not isinstance(intentions, list):
            raise ValueError("Intentions must be a non-empty list of lists.")

        query_blocks = []

        for group in intentions:
            if not group:
                continue

            for intent in group:
                # Build single intent filter
                intention_filter = f"insight_type ILIKE '%%{intent}%%'"

                # Optional FA filter
                fa_filter = ""
                if fa_name and fa_name != "FA_NOT_MENTIONED":
                    fa_filter = f" AND array_to_string(fa_names, ',') ILIKE '%%{fa_name}%%'"

                # Build SELECT block with internal limit
                query_block = f"""
                    SELECT 
                        insight_type, fa_names, acc_nm, included_clients,
                        REGEXP_REPLACE(narrative_raw, '<[^>]+>', '', 'g') AS narrative_raw
                    FROM {settings.psql_schema}.{settings.psql_table}
                    WHERE {intention_filter}{fa_filter}
                    LIMIT 10
                """
                query_blocks.append(query_block.strip())

        # Join all SELECTs using UNION ALL
        combined_query = "\nUNION ALL\n".join(query_blocks)

        return combined_query.strip()

    except Exception as e:
        logger.exception(f"Error in building combined query. Error: {e}")
        return None
-------------------------------------------------------------------------------------------

def build_combined_query(intentions, conjunction=None, fa_name=None, total_limit=None):
    try:
        if not intentions or not isinstance(intentions, list):
            raise ValueError("Intentions must be a non-empty list of lists.")

        query_blocks = []

        for group in intentions:
            if not group:
                continue

            # Build the OR clause for this intention group
            group_clause = " OR ".join([f"insight_type ILIKE '%%{intent}%%'" for intent in group])
            intention_filter = f"({group_clause})"

            # Optional FA filter
            fa_filter = ""
            if fa_name and fa_name != "FA_NOT_MENTIONED":
                fa_filter = f" AND array_to_string(fa_names, ',') ILIKE '%%{fa_name}%%'"

            # Build this SELECT block
            query_block = f"""
                SELECT 
                    insight_type, fa_names, acc_nm, included_clients,
                    REGEXP_REPLACE(narrative_raw, '<[^>]+>', '', 'g') AS narrative_raw
                FROM {settings.pgsql_schema}.{settings.pgsql_table}
                WHERE {intention_filter}{fa_filter}
                LIMIT 10
            """
            query_blocks.append(query_block.strip())

        # Combine with UNION ALL
        combined_query = "\nUNION ALL\n".join(query_blocks)

        # Optionally apply total LIMIT
        if total_limit:
            combined_query = f"""
                SELECT * FROM (
                    {combined_query}
                ) AS combined_results
                LIMIT {total_limit}
            """

        return combined_query.strip()

    except Exception as e:
        logger.exception(f"Error in building combined query. Error: {e}")
        return None
-----------------------------------------------------------------------------------------------------------------------------------------


def build_combined_query(intentions, conjunction=None, fa_name=None, total_limit=None):
    """
    Build a combined SQL query using UNION ALL across intention groups.

    Args:
        intentions (list of lists): Each sublist contains intention keywords.
        conjunction (str): Unused in UNION mode but kept for compatibility.
        fa_name (str): Optional FA name filter.
        total_limit (int): Optional limit on total rows returned.

    Returns:
        str: Final SQL query string.
    """
    try:
        if not intentions or not isinstance(intentions, list):
            raise ValueError("Intentions must be a non-empty list of lists.")

        queries = []

        for group in intentions:
            if not group:
                continue

            # Build the OR clause for this group
            group_clause = " OR ".join([f"insight_type ILIKE '%{intent}%'" for intent in group])
            intention_filter = f"({group_clause})"

            # Add FA filter if applicable
            fa_filter = ""
            if fa_name and fa_name != "FA_NOT_MENTIONED":
                fa_filter = f" AND array_to_string(fa_names, ',') ILIKE '%{fa_name}%'"

            # Final WHERE clause
            where_clause = f"WHERE {intention_filter}{fa_filter}"

            # Single group query (limit per group)
            query = f"""
                SELECT 
                    insight_type, fa_names, acc_nm, included_clients,
                    REGEXP_REPLACE(narrative_raw, '<[^>]+>', '', 'g') AS narrative_raw
                FROM {settings.pgsql_schema}.{settings.pgsql_table}
                {where_clause}
                LIMIT 10
            """
            queries.append(query.strip())

        # Combine all with UNION ALL
        union_query = "\nUNION ALL\n".join(queries)

        # Optional total limit wrapper
        if total_limit:
            final_query = f"""
                SELECT * FROM (
                    {union_query}
                ) AS combined_results
                LIMIT {total_limit}
            """
        else:
            final_query = union_query

        print("Generated SQL:\n", final_query)  # Optional: remove in prod
        return final_query

    except Exception as e:
        logger.exception(f"Error in building combined query. Error: {e}")
        return None


--------------------------------------------------------
SELECT REGEXP_REPLACE(your_column, '<[^>]*>', '', 'g') AS cleaned_text
FROM your_table;✅

Step-by-Step: Setup with Conda + Python 3.11
🔹 Step 1: Create a Conda Environment with Python 3.11
In your terminal:

bash
Copy
Edit
conda create -n promptwizard-env python=3.11 -y
Activate it:

bash
Copy
Edit
conda activate promptwizard-env
🔹 Step 2: Set Up Your Project Structure
Ensure your folder looks like this:

arduino
Copy
Edit
PromptWizard/
├── promptwizard/
│   ├── __init__.py        ← Your PromptWizard class lives here
├── pythonProject/
│   └── test.py            ← Testing script
├── setup.py
🔹 Step 3: Create the setup.py File
In the root PromptWizard/ folder, create setup.py:

python
Copy
Edit
# setup.py

from setuptools import setup, find_packages

setup(
    name='promptwizard',
    version='0.1',
    packages=find_packages(),
    description='A lightweight prompt template runner',
    author='Your Name',
    install_requires=[],
)
🔹 Step 4: Install the Package in Editable Mode
Inside the promptwizard-env Conda environment:

bash
Copy
Edit
pip install -e .
Even in Conda, pip works fine for installing editable packages.

🔹 Step 5: Run Your Test Script
Make sure you're still in the promptwizard-env environment and run:

bash
Copy
Edit
python pythonProject/test.py
🧠 Recap
You now have:

✅ A Conda environment named promptwizard-env

✅ Python version 3.11

✅ A locally installable prompt package using pip install -e .


query = f"""
    SELECT 
        REGEXP_REPLACE(RIGHT(LEFT(global_key, LENGTH(global_key)-13), LENGTH(global_key)-13), '[^a-zA-Z0-9_]', '', 'g') AS global_key,
        REGEXP_REPLACE(target_type, '[^a-zA-Z0-9_]', '', 'g') AS target_type,
        REGEXP_REPLACE(target_key, '[^a-zA-Z0-9_]', '', 'g') AS target_key,
        acc_mhh_n, insight_type,
        ARRAY_TO_STRING(fa_names, '|') AS fa_names,
        ARRAY_TO_STRING(fa_ids, '|') AS fa_ids,
        narrative_title_plain,
        narrative_raw
    FROM sandbox_whared.si_nlg.fa_narratives_review_curr
    WHERE MOD(ABS(HASHTEXT(global_key)), 16) = {key_mod}
"""

03/07
class PromptWizard:
    def __init__(self):
        print("PromptWizard initialized")

    def run(self):
        print("Running PromptWizard logic")
Then your test.py will work with:

python
Copy
Edit
from promptwizard import PromptWizard

wizard = PromptWizard()
wizard.run()

from promptwizard import PromptWizard

def main():
    # Initialize PromptWizard
    wizard = PromptWizard()

    # Set your prompt manually (or read from file)
    prompt = """
    You are an expert summarizer. Summarize the following text in one paragraph:

    TEXT:
    Artificial Intelligence is a field of computer science that enables machines to mimic human intelligence...
    """

    # Run PromptWizard with the prompt
    response = wizard.run(prompt)

    # Print the response
    print("Generated Output:\n", response)

if __name__ == "__main__":
    main()

02/07

import math
import pandas as pd

# Define batch size
batch_size = 1000
total_rows = len(df)

# Loop over DataFrame in batches
for i in range(0, total_rows, batch_size):
    # Get the current batch
    batch = df.iloc[i:i + batch_size].copy()
    
    # Clean and prepare the input text (if needed)
    batch["narrative_raw_plain"] = batch["narrative_raw"].apply(remove_html_tags)
    texts = batch["narrative_raw_plain"].fillna("").tolist()  # Avoid NaNs
    
    # Compute embeddings in batch using embed_documents
    batch["embedding_list"] = embeddings.embed_documents(texts)

# Safely assign back
    batch["embedding"] = embeddings_list

    # Optional print for tracking
    print(f"Embedding completed for batch {i // batch_size + 1}")

    # Write to database or wherever needed
    helper.write_to_db(batch, db_settings)



23/06
SELECT
    array_to_string(fa_names, '') AS fa_names,
    insight_type,
    array_to_string(
        ARRAY(
            SELECT fa || ' -> ' || insight_type
            FROM unnest(fa_names) AS fa
        ),
        ', '
    ) AS fa_insight_mapping
FROM
    sandbox_wma_shared.si_nlg_fa_narratives_review_curr2;


SELECT
    fa AS fa_name,
    insight_type
FROM
    sandbox_wma_shared.si_nlg_fa_narratives_review_curr2,
    UNNEST(fa_names) AS fa;


SELECT
    fa AS fa_name,
    ARRAY_AGG(DISTINCT insight_type) AS insight_types
FROM
    sandbox_wma_shared.si_nlg_fa_narratives_review_curr2,
    UNNEST(fa_names) AS fa
GROUP BY
    fa;


SELECT
    insight_type,
    ARRAY_AGG(DISTINCT fa) AS fa_names
FROM
    sandbox_wma_shared.si_nlg_fa_narratives_review_curr2,
    UNNEST(fa_names) AS fa
GROUP BY
    insight_type;

----------------------------------------------------------------------

WITH unnested_fa_names AS (
    SELECT unnest(fa_names) AS fa_name
    FROM sandbox_wma_shared.si_nlg_fa_narratives_review_curr2
),
fa_name_counts AS (
    SELECT fa_name, COUNT(*) AS cnt
    FROM unnested_fa_names
    GROUP BY fa_name
)
SELECT 
    fa_name,
    cnt AS occurrence_count
FROM fa_name_counts
WHERE cnt < 5
ORDER BY cnt ASC, fa_name;
